{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11742785,"sourceType":"datasetVersion","datasetId":7078972}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1 - Import requirements","metadata":{}},{"cell_type":"code","source":"# !pip install pytorch-optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T06:48:23.538587Z","iopub.execute_input":"2025-05-08T06:48:23.538795Z","iopub.status.idle":"2025-05-08T06:48:23.542473Z","shell.execute_reply.started":"2025-05-08T06:48:23.538773Z","shell.execute_reply":"2025-05-08T06:48:23.541553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport math\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torch.nn.functional as F\n# from pytorch_optimizer import SAM\n\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2025-05-09T06:44:49.906609Z","iopub.execute_input":"2025-05-09T06:44:49.906876Z","iopub.status.idle":"2025-05-09T06:44:55.513928Z","shell.execute_reply.started":"2025-05-09T06:44:49.906848Z","shell.execute_reply":"2025-05-09T06:44:55.513242Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2 - Prepare data","metadata":{}},{"cell_type":"code","source":"label_mapping = {\n    'BUY': 0,\n    'SELL': 1,\n    'HOLD': 2\n}\n\ndef map_label(x):\n    return label_mapping[x] if x in label_mapping else x","metadata":{"execution":{"iopub.status.busy":"2025-05-09T06:45:15.394845Z","iopub.execute_input":"2025-05-09T06:45:15.395180Z","iopub.status.idle":"2025-05-09T06:45:15.399672Z","shell.execute_reply.started":"2025-05-09T06:45:15.395122Z","shell.execute_reply":"2025-05-09T06:45:15.398713Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_shape(shape_path):\n    with open(shape_path, 'r') as f:\n        shape = f.readlines()\n        n_samples = int(shape[0])\n        seq_len = int(shape[1])\n        n_features = int(shape[2])\n    return n_samples, seq_len, n_features\n\nclass TradingDataset(Dataset):\n    def __init__(self, save_path, n_samples, sequence_length, n_features):\n        self.save_path = save_path\n        self.n_samples = n_samples\n        self.sequences = np.memmap(f'{save_path}/sequences.dat', dtype=np.float32, mode='r', \n                                 shape=(n_samples, sequence_length, n_features))\n        # self.sequences = self.sequences[:,:-10]\n        self.labels = np.memmap(f'{save_path}/labels.dat', dtype=np.int64, mode='r', \n                              shape=(n_samples,))\n    \n    def __len__(self):\n        return self.n_samples\n    \n    def __getitem__(self, idx):\n        seq = self.sequences[idx].copy()  # Tạo bản sao writable\n        lbl = self.labels[idx].copy()\n        return torch.from_numpy(seq).float(), torch.from_numpy(np.array([lbl])).long()[0]\n\ndef prepare_transformer_input(train_shape_path, val_shape_path, test_shape_path, data_path, batch_size=32):    \n    n_train_samples, sequence_length, n_features = load_shape(train_shape_path)\n    n_val_samples, _, _ = load_shape(val_shape_path)\n    n_test_samples, _, _ = load_shape(test_shape_path)\n    \n    train_path = f'{data_path}/train'\n    val_path = f'{data_path}/val'\n    test_path = f'{data_path}/test'\n    \n    # Tạo datasets\n    train_dataset = TradingDataset(train_path, n_train_samples, sequence_length, n_features)\n    val_dataset = TradingDataset(val_path, n_val_samples, sequence_length, n_features)\n    test_dataset = TradingDataset(test_path, n_test_samples, sequence_length, n_features)\n    \n    # Tạo dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    return train_loader, val_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2025-05-09T06:45:17.109556Z","iopub.execute_input":"2025-05-09T06:45:17.109845Z","iopub.status.idle":"2025-05-09T06:45:17.117964Z","shell.execute_reply.started":"2025-05-09T06:45:17.109824Z","shell.execute_reply":"2025-05-09T06:45:17.117079Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_shape_path = '/kaggle/input/processed-xauusd/xau5m/train/shape.txt'\nval_shape_path = '/kaggle/input/processed-xauusd/xau5m/val/shape.txt'\ntest_shape_path = '/kaggle/input/processed-xauusd/xau5m/test/shape.txt'\n\ntrain_loader, val_loader, test_loader = prepare_transformer_input(\n    train_shape_path, val_shape_path, test_shape_path,\n    data_path='/kaggle/input/processed-xauusd/xau5m',\n    batch_size=32\n)","metadata":{"execution":{"iopub.status.busy":"2025-05-09T06:46:22.874343Z","iopub.execute_input":"2025-05-09T06:46:22.874648Z","iopub.status.idle":"2025-05-09T06:46:22.903965Z","shell.execute_reply.started":"2025-05-09T06:46:22.874627Z","shell.execute_reply":"2025-05-09T06:46:22.903156Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Kiểm tra\nsample_batch = next(iter(train_loader))\nprint(\"Batch input shape:\", sample_batch[0].shape)\nprint(\"Batch labels shape:\", sample_batch[1].shape)\nprint(\"\\nExample input shape for Transformer:\", sample_batch[0][0].shape)\nprint(sample_batch[0][0])\nprint(\"Number of batches:\", len(train_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T06:46:24.180670Z","iopub.execute_input":"2025-05-09T06:46:24.180952Z","iopub.status.idle":"2025-05-09T06:46:24.340010Z","shell.execute_reply.started":"2025-05-09T06:46:24.180933Z","shell.execute_reply":"2025-05-09T06:46:24.338948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3 - Build model","metadata":{}},{"cell_type":"code","source":"class InceptionModule(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.branch1 = nn.Conv1d(in_channels, 32, kernel_size=1, padding='same')\n        self.branch3 = nn.Conv1d(in_channels, 32, kernel_size=3, padding='same')\n        self.branch5 = nn.Conv1d(in_channels, 32, kernel_size=5, padding='same')\n        self.branch_pool = nn.Sequential(\n            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n            nn.Conv1d(in_channels, 32, kernel_size=1)\n        )\n\n    def forward(self, x):\n        return torch.cat([self.branch1(x), self.branch3(x), self.branch5(x), self.branch_pool(x)], dim=1)\n\nclass Time2Vec(nn.Module):\n    def __init__(self, kernel_size, max_len=512):\n        super().__init__()\n        self.linear = nn.Linear(1, 1)\n        self.periodic = nn.Linear(1, kernel_size - 1)\n        self.register_buffer('t', torch.arange(max_len).float().unsqueeze(1)) # [max_len, 1]\n\n    def forward(self, x):\n        # x = [batch_size, seq_len, n_features]\n        seq_len = x.size(1)\n        t = self.t[:seq_len] # [seq_len, 1]\n        lin = self.linear(t) # [seq_len, 1]\n        sin = torch.sin(self.periodic(t)) # [seq_len, k - 1]\n        time_emb = torch.cat([lin, sin], dim=1) # [seq_len, k]\n        time_emb = time_emb.unsqueeze(0).repeat(x.size(0), 1, 1) # [batch_size, seq_len, k]\n        return torch.cat([x, time_emb], dim=-1)\n        # return x + time_emb\n\nclass CrossAttentionFusion(nn.Module):\n    def __init__(self, cnn_dim, transformer_dim):\n        super().__init__()\n        self.query = nn.Linear(cnn_dim, transformer_dim)\n        self.key = nn.Linear(transformer_dim, transformer_dim)\n        self.value = nn.Linear(transformer_dim, transformer_dim)\n        \n    def forward(self, cnn_features, transformer_features):\n        Q = self.query(cnn_features).unsqueeze(1)  # [batch, 1, transformer_dim]\n        K = self.key(transformer_features)         # [batch, seq_len, transformer_dim]\n        V = self.value(transformer_features)       # [batch, seq_len, transformer_dim]\n        \n        attn_scores = (Q @ K.transpose(-2, -1)) / (K.size(-1) ** 0.5)  # [batch, 1, seq_len]\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        \n        return torch.bmm(attn_weights, V).squeeze(1)  # [batch, transformer_dim]\n\nclass HighwayNetwork(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.gate = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, fused, transformer):\n        g = self.gate(fused)\n        return g * fused + (1 - g) * transformer\n\nclass EnhancedHybridModel(nn.Module):\n    def __init__(self, num_features, num_classes=3, d_model=512, nhead=16, dim_feedforward=1024, num_layers=6):\n        super().__init__()\n        # 1. InceptionTime Branch\n        self.inception = nn.Sequential(\n            InceptionModule(num_features),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n            InceptionModule(128),\n            nn.ReLU()\n        )\n        \n        # 2. Transformer Branch\n        self.time2vec = Time2Vec(num_features)\n        self.transformer_proj = nn.Linear(num_features * 2, d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        # 3. Fusion\n        self.cross_attention = CrossAttentionFusion(128, d_model)\n        self.highway = HighwayNetwork(d_model)\n        \n        # 4. Classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(d_model, 128),\n            nn.LayerNorm(128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        # 1. Inception Path\n        cnn_features = self.inception(x.permute(0, 2, 1))  # [batch, channels, seq_len//2]\n        cnn_features = cnn_features.mean(dim=-1)          # [batch, channels=128]\n        \n        # 2. Transformer Path\n        x_proj = self.transformer_proj(self.time2vec(x))  # [batch, seq_len, d_model]\n        transformer_features = self.transformer(x_proj)      # [batch, seq_len, d_model]\n        \n        # 3. Fusion\n        fused = self.cross_attention(cnn_features, transformer_features)  # [batch, d_model]\n        output = self.highway(fused, transformer_features.mean(dim=1))   # [batch, d_model]\n        \n        return self.classifier(output)","metadata":{"execution":{"iopub.status.busy":"2025-05-09T06:46:30.804851Z","iopub.execute_input":"2025-05-09T06:46:30.805186Z","iopub.status.idle":"2025-05-09T06:46:30.820309Z","shell.execute_reply.started":"2025-05-09T06:46:30.805126Z","shell.execute_reply":"2025-05-09T06:46:30.819183Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"N_FEATURES = 15\nmodel = EnhancedHybridModel(num_features=N_FEATURES) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T06:46:36.257957Z","iopub.execute_input":"2025-05-09T06:46:36.258299Z","iopub.status.idle":"2025-05-09T06:46:36.327669Z","shell.execute_reply.started":"2025-05-09T06:46:36.258272Z","shell.execute_reply":"2025-05-09T06:46:36.326740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class Time2Vec(nn.Module):\n#     def __init__(self, k, max_len=512):\n#         super().__init__()\n#         self.linear = nn.Linear(1, 1)\n#         self.periodic = nn.Linear(1, k - 1)\n#         self.register_buffer('t', torch.arange(max_len).float().unsqueeze(1)) # [max_len, 1]\n\n#     def forward(self, x):\n#         # x = [batch_size, seq_len, n_features]\n#         seq_len = x.size(1)\n#         t = self.t[:seq_len] # [seq_len, 1]\n#         lin = self.linear(t) # [seq_len, 1]\n#         sin = torch.sin(self.periodic(t)) # [seq_len, k - 1]\n#         time_emb = torch.cat([lin, sin], dim=1) # [seq_len, k]\n#         time_emb = time_emb.unsqueeze(0).repeat(x.size(0), 1, 1) # [batch_size, seq_len, k]\n#         return torch.cat([x, time_emb], dim=-1)\n\n# class TradingTF(nn.Module):\n#     def __init__(self, seq_len, num_classes, input_dim, time2vec_dim, d_model, nhead, num_layers, dropout=0.5):\n#         super().__init__()\n#         self.time2vec = Time2Vec(time2vec_dim)\n#         self.input_projection = nn.Linear(input_dim + time2vec_dim, d_model)\n#         encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n#         self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n#         self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n#         self.dropout = nn.Dropout(dropout)\n#         self.fc1 = nn.Linear(d_model, 32)\n#         self.fc2 = nn.Linear(32, num_classes)\n\n#     def forward(self, x):\n#         x = self.time2vec(x)\n#         x = self.input_projection(x)\n#         x = self.encoder(x)\n#         x = x.transpose(1, 2)\n#         x = self.global_avg_pool(x).squeeze(-1)\n#         x = self.dropout(x)\n#         x = F.relu((self.fc1(x)))\n#         x = self.dropout(x)\n#         return self.fc2(x)\n\n# model = TradingTF(seq_len=128, num_classes=3, input_dim=21, time2vec_dim=16, d_model=128, nhead=4, num_layers=2, dropout=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:13:32.387946Z","iopub.execute_input":"2025-05-08T08:13:32.388268Z","iopub.status.idle":"2025-05-08T08:13:32.392047Z","shell.execute_reply.started":"2025-05-08T08:13:32.388242Z","shell.execute_reply":"2025-05-08T08:13:32.391020Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model(sample_batch[0]).shape, sample_batch[1].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T06:46:40.386599Z","iopub.execute_input":"2025-05-09T06:46:40.386928Z","iopub.status.idle":"2025-05-09T06:46:41.204814Z","shell.execute_reply.started":"2025-05-09T06:46:40.386905Z","shell.execute_reply":"2025-05-09T06:46:41.204115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchinfo import summary\nprint(summary(model, (32, 60, 15)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T06:46:49.913111Z","iopub.execute_input":"2025-05-09T06:46:49.913465Z","iopub.status.idle":"2025-05-09T06:46:50.922339Z","shell.execute_reply.started":"2025-05-09T06:46:49.913442Z","shell.execute_reply":"2025-05-09T06:46:50.921457Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4 - Train and Evaluate model","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=3, min_delta=0.001):\n        \"\"\"\n        patience: Số epoch chờ mà không cải thiện trước khi dừng\n        min_delta: Độ cải thiện tối thiểu để coi là tốt hơn\n        \"\"\"\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif val_loss > self.best_loss - self.min_delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.counter = 0\n            \ndef eval_model(model, val_loader, criterion, device):\n    model.to(device)\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    loop = tqdm(val_loader, unit='batch', desc='\\tEvaluating: ')\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(loop):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, -1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            loop.set_postfix(loss=(running_loss / (i + 1)))\n    \n    epoch_loss = running_loss / len(val_loader)\n    epoch_acc = 100 * correct / total\n    \n    return epoch_loss, epoch_acc\n\ndef train_model(model, train_loader, criterion, optimizer, device, scheduler=None):\n    model.to(device)\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    loop = tqdm(train_loader, unit='batch', desc=f'\\tTraining: ')\n    for i, (images, labels) in enumerate(loop):\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, -1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        loop.set_postfix(loss=(running_loss / (i + 1)))\n    \n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = 100 * correct / total\n\n    if scheduler is not None:\n        scheduler.step()\n    \n    return epoch_loss, epoch_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:13:38.499631Z","iopub.execute_input":"2025-05-08T08:13:38.499980Z","iopub.status.idle":"2025-05-08T08:13:38.510703Z","shell.execute_reply.started":"2025-05-08T08:13:38.499951Z","shell.execute_reply":"2025-05-08T08:13:38.509684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nn_train_samples, _, _ = load_shape('/kaggle/input/processed-xauusd/classification/train/shape.txt')\ntrain_labels = np.memmap(f'/kaggle/input/processed-xauusd/classification/train/labels.dat', dtype=np.int64, mode='r', \n                              shape=(n_train_samples,))\n# class_weights = compute_class_weight(\n#     'balanced', \n#     classes=np.unique(train_labels), \n#     y=train_labels\n# )\n# criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(DEVICE))\ncriterion = nn.CrossEntropyLoss(ignore_index=2)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:13:41.415026Z","iopub.execute_input":"2025-05-08T08:13:41.415345Z","iopub.status.idle":"2025-05-08T08:13:42.086929Z","shell.execute_reply.started":"2025-05-08T08:13:41.415319Z","shell.execute_reply":"2025-05-08T08:13:42.086061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_EPOCHS = 50\nPATIENCE = 10\nMIN_DELTA = 0.0001\ntorch.cuda.empty_cache()\n\n# model = nn.DataParallel(model)\n    \ntrain_losses = []\ntrain_accs = []\nval_losses = []\nval_accs = []\n\nearly_stopping = EarlyStopping(patience=PATIENCE, min_delta=MIN_DELTA)\n\nfor epoch in range(NUM_EPOCHS):\n    print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}]')\n    \n    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, DEVICE)\n    val_loss, val_acc = eval_model(model, val_loader, criterion, DEVICE)\n    \n    print(f'\\tTrain Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%')\n    print(f'\\tVal Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%')\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n    \n    # Kiểm tra Early Stopping\n    early_stopping(val_loss)\n    if early_stopping.early_stop:\n        print(\"Early stopping triggered!\")\n        break\n    print('===================================================')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:13:42.227988Z","iopub.execute_input":"2025-05-08T08:13:42.228283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_training_results(train_losses, train_accs, val_losses, val_accs):\n    \"\"\"\n    Vẽ biểu đồ kết quả huấn luyện: loss và accuracy cho train và validation.\n    \n    Parameters:\n    - train_losses: List các giá trị loss của train qua các epoch\n    - train_accs: List các giá trị accuracy của train qua các epoch\n    - val_losses: List các giá trị loss của validation qua các epoch\n    - val_accs: List các giá trị accuracy của validation qua các epoch\n    \"\"\"\n    epochs = range(1, len(train_losses) + 1)\n    \n    # Tạo figure với 2x2 subplot\n    plt.figure(figsize=(12, 8))\n    \n    # Subplot 1: Train Loss\n    plt.subplot(2, 2, 1)\n    plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # Subplot 2: Train Accuracy\n    plt.subplot(2, 2, 2)\n    plt.plot(epochs, train_accs, 'g-', label='Train Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.title('Training Accuracy')\n    plt.legend()\n    plt.grid(True)\n    \n    # Subplot 3: Validation Loss\n    plt.subplot(2, 2, 3)\n    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Validation Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # Subplot 4: Validation Accuracy\n    plt.subplot(2, 2, 4)\n    plt.plot(epochs, val_accs, 'm-', label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.title('Validation Accuracy')\n    plt.legend()\n    plt.grid(True)\n    \n    # Điều chỉnh layout và hiển thị\n    plt.tight_layout()\n    plt.show()\n\nplot_training_results(train_losses, train_accs, val_losses, val_accs)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T05:10:33.388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model.pth')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T05:10:33.388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(model, test_loader, device='cuda' if torch.cuda.is_available() else 'cpu'):\n    model.to(device)\n    model.eval()\n\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch_x, batch_y in test_loader:\n            batch_x = batch_x.to(device)  # [B, seq_len, n_features]\n            batch_y = batch_y.to(device)\n\n            outputs = model(batch_x)  # Expecting [B, num_classes]\n            preds = torch.argmax(outputs, dim=-1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch_y.cpu().numpy())\n\n    target_names = ['BUY', 'SELL', 'HOLD']\n    cm = confusion_matrix(all_labels, all_preds)\n    print('Confusion matrix:')\n    print(pd.DataFrame(cm, columns=target_names, index=target_names))\n    print(\"Classification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=target_names, digits=4, zero_division=0))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T05:10:33.388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(model, train_loader, DEVICE)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T05:10:33.388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(model, val_loader, DEVICE)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T05:10:33.388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(model, test_loader, DEVICE)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T05:10:33.388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_prediction(model, inputs, device):\n    # inputs = [B, S, N]\n    model.to(device)\n    model.eval()\n    with torch.no_grad():\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        preds = torch.argmax(outputs, dim=-1)\n    return preds.cpu().numpy()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T05:10:33.388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch = next(iter(test_loader))\ninputs, labels = batch[0], batch[1]\n\nprint(\"Prediction: \", get_prediction(model, inputs, DEVICE))\nprint(\"Ground Truth: \", labels.cpu().numpy())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T05:10:33.388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}