{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a16c60",
   "metadata": {},
   "source": [
    "# 1 - Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3215f98a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T15:49:22.550173Z",
     "iopub.status.busy": "2025-04-06T15:49:22.549784Z",
     "iopub.status.idle": "2025-04-06T15:49:26.899296Z",
     "shell.execute_reply": "2025-04-06T15:49:26.898256Z",
     "shell.execute_reply.started": "2025-04-06T15:49:22.550106Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be178d",
   "metadata": {},
   "source": [
    "# 2 - Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e540e20-ab23-474d-9626-aa733e13b49b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T15:49:36.395369Z",
     "iopub.status.busy": "2025-04-06T15:49:36.395044Z",
     "iopub.status.idle": "2025-04-06T15:49:38.330351Z",
     "shell.execute_reply": "2025-04-06T15:49:38.329291Z",
     "shell.execute_reply.started": "2025-04-06T15:49:36.395330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'BUY': 0,\n",
    "    'SELL': 1,\n",
    "    'HOLD': 2\n",
    "}\n",
    "\n",
    "def map_label(x):\n",
    "    return label_mapping[x] if x in label_mapping else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb91cd",
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-06T16:06:18.186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_shape(shape_path):\n",
    "    with open(shape_path, 'r') as f:\n",
    "        shape = f.readlines()\n",
    "        n_samples = int(shape[0])\n",
    "        seq_len = int(shape[1])\n",
    "        n_features = int(shape[2])\n",
    "    return n_samples, seq_len, n_features\n",
    "\n",
    "class TradingDataset(Dataset):\n",
    "    def __init__(self, save_path, n_samples, sequence_length, n_features):\n",
    "        self.save_path = save_path\n",
    "        self.n_samples = n_samples\n",
    "        self.sequences = np.memmap(f'{save_path}/sequences.dat', dtype=np.float32, mode='r', \n",
    "                                 shape=(n_samples, sequence_length, n_features))\n",
    "        self.labels = np.memmap(f'{save_path}/labels.dat', dtype=np.int64, mode='r', \n",
    "                              shape=(n_samples,))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx].copy()  # Tạo bản sao writable\n",
    "        lbl = self.labels[idx].copy()\n",
    "        return torch.from_numpy(seq).float(), torch.from_numpy(np.array([lbl])).long()[0]\n",
    "\n",
    "def prepare_transformer_input(train_shape_path, val_shape_path, test_shape_path, data_path, batch_size=32):    \n",
    "    n_train_samples, sequence_length, n_features = load_shape(train_shape_path)\n",
    "    n_val_samples, _, _ = load_shape(val_shape_path)\n",
    "    n_test_samples, _, _ = load_shape(test_shape_path)\n",
    "    \n",
    "    train_path = f'{data_path}/train'\n",
    "    val_path = f'{data_path}/val'\n",
    "    test_path = f'{data_path}/test'\n",
    "    \n",
    "    # Tạo datasets\n",
    "    train_dataset = Subset(TradingDataset(train_path, n_train_samples, sequence_length, n_features), \n",
    "                           range(n_train_samples - 100000, n_train_samples))\n",
    "    val_dataset = Subset(TradingDataset(val_path, n_val_samples, sequence_length, n_features), \n",
    "                           range(n_val_samples - 10000, n_val_samples))\n",
    "    test_dataset = Subset(TradingDataset(test_path, n_test_samples, sequence_length, n_features), \n",
    "                           range(n_test_samples - 1000, n_test_samples))\n",
    "    \n",
    "    # Tạo dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd11f2df-64e5-4a69-b2e8-f44d117fee49",
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-06T16:06:18.186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_shape_path = 'data/train/train_shape.txt'\n",
    "val_shape_path = 'data/val/val_shape.txt'\n",
    "test_shape_path = 'data/test/test_shape.txt'\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_transformer_input(\n",
    "    train_shape_path, val_shape_path, test_shape_path,\n",
    "    data_path='data',\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cd830e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input shape: torch.Size([64, 60, 24])\n",
      "Batch labels shape: torch.Size([64])\n",
      "\n",
      "Example input shape for Transformer: torch.Size([60, 24])\n",
      "tensor([[ 1.4084e+03,  1.4089e+03,  1.3930e+03,  ..., -9.6593e-01,\n",
      "          7.8183e-01,  6.2349e-01],\n",
      "        [ 1.3953e+03,  1.3971e+03,  1.3854e+03,  ..., -8.6603e-01,\n",
      "          7.8183e-01,  6.2349e-01],\n",
      "        [ 1.3876e+03,  1.3890e+03,  1.3816e+03,  ..., -7.0711e-01,\n",
      "          7.8183e-01,  6.2349e-01],\n",
      "        ...,\n",
      "        [ 1.3674e+03,  1.3674e+03,  1.3651e+03,  ..., -2.5882e-01,\n",
      "         -4.3388e-01, -9.0097e-01],\n",
      "        [ 1.3654e+03,  1.3665e+03,  1.3605e+03,  ..., -5.0000e-01,\n",
      "         -4.3388e-01, -9.0097e-01],\n",
      "        [ 1.3629e+03,  1.3630e+03,  1.3566e+03,  ..., -7.0711e-01,\n",
      "         -4.3388e-01, -9.0097e-01]])\n",
      "Number of batches: 53851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\longt\\AppData\\Local\\Temp\\ipykernel_5864\\2313994758.py:68: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  return (torch.from_numpy(self.sequences[idx]).float(),\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"Batch input shape:\", sample_batch[0].shape)\n",
    "print(\"Batch labels shape:\", sample_batch[1].shape)\n",
    "print(\"\\nExample input shape for Transformer:\", sample_batch[0][0].shape)\n",
    "print(sample_batch[0][0])\n",
    "print(\"Number of batches:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c95237f8-ac3c-47f5-b7da-e3b9fd3ba6b4",
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-06T16:06:18.186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "class TradingTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_features, \n",
    "                 num_classes=3, \n",
    "                 d_model=64,\n",
    "                 nhead=8,\n",
    "                 num_layers=3,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Feature Projection\n",
    "        self.input_proj = nn.Linear(num_features, d_model)\n",
    "        \n",
    "        # 2. Positional Encoding (Learnable)\n",
    "        self.pos_encoder = LearnablePositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # 3. Transformer Encoder\n",
    "        encoder_layers = TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model*4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True \n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        # 4. Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model//2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 5. Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor shape [batch_size, seq_len, num_features]\n",
    "        Returns:\n",
    "            output: Tensor shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        # Project input features\n",
    "        x = self.input_proj(src)  # [B, S, D]\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Transformer processing\n",
    "        memory = self.transformer_encoder(x)  # [B, S, D]\n",
    "        \n",
    "        # Get last time step output\n",
    "        last_output = memory[:, -1, :]  # [B, D]\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(last_output)\n",
    "\n",
    "\n",
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.position_emb = nn.Parameter(torch.zeros(max_len, d_model))\n",
    "        nn.init.normal_(self.position_emb, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor shape [B, S, D]\n",
    "        \"\"\"\n",
    "        positions = self.position_emb[:x.size(1), :]  # [S, D]\n",
    "        x = x + positions.unsqueeze(0)  # [B, S, D]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "652895ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3]), torch.Size([64]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_FEATURES = 24\n",
    "model = TradingTransformer(N_FEATURES)\n",
    "\n",
    "model(sample_batch[0]).shape, sample_batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d45736e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "TradingTransformer                            [64, 3]                   --\n",
      "├─Linear: 1-1                                 [64, 60, 64]              1,600\n",
      "├─LearnablePositionalEncoding: 1-2            [64, 60, 64]              320,000\n",
      "│    └─Dropout: 2-1                           [64, 60, 64]              --\n",
      "├─TransformerEncoder: 1-3                     [64, 60, 64]              --\n",
      "│    └─ModuleList: 2-2                        --                        --\n",
      "│    │    └─TransformerEncoderLayer: 3-1      [64, 60, 64]              49,984\n",
      "│    │    └─TransformerEncoderLayer: 3-2      [64, 60, 64]              49,984\n",
      "│    │    └─TransformerEncoderLayer: 3-3      [64, 60, 64]              49,984\n",
      "├─Sequential: 1-4                             [64, 3]                   --\n",
      "│    └─Linear: 2-3                            [64, 32]                  2,080\n",
      "│    └─ReLU: 2-4                              [64, 32]                  --\n",
      "│    └─Dropout: 2-5                           [64, 32]                  --\n",
      "│    └─Linear: 2-6                            [64, 3]                   99\n",
      "===============================================================================================\n",
      "Total params: 473,731\n",
      "Trainable params: 473,731\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.24\n",
      "===============================================================================================\n",
      "Input size (MB): 0.37\n",
      "Forward/backward pass size (MB): 1.98\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 2.37\n",
      "===============================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\longt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:720: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "print(summary(model, (64, 60, 24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9760660",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        \"\"\"\n",
    "        patience: Số epoch chờ mà không cải thiện trước khi dừng\n",
    "        min_delta: Độ cải thiện tối thiểu để coi là tốt hơn\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            \n",
    "def eval_model(model, val_loader, criterion, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, unit='batch', desc='\\tEvaluating: '):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    print(f'\\tValidation Loss: {epoch_loss:.4f}, Validation Accuracy: {epoch_acc:.2f}%')\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, patience=3):\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=0.001)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, unit='batch', desc=f'Training [{epoch + 1}/{num_epochs}]: '):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "        print(f'\\tTrain Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.2f}%')\n",
    "\n",
    "        val_loss, val_acc = eval_model(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Kiểm tra Early Stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "        print('===================================================')\n",
    "    \n",
    "    return train_losses, train_accs, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8adff6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5423930c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [1/20]: 100%|██████████| 53851/53851 [15:17<00:00, 58.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.0288, Train Accuracy: 49.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEvaluating: 100%|██████████| 5483/5483 [00:30<00:00, 179.51batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation Loss: 1.0565, Validation Accuracy: 45.96%\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [2/20]: 100%|██████████| 53851/53851 [15:58<00:00, 56.15batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.0268, Train Accuracy: 49.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEvaluating: 100%|██████████| 5483/5483 [00:32<00:00, 169.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation Loss: 1.0553, Validation Accuracy: 45.96%\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [3/20]: 100%|██████████| 53851/53851 [15:57<00:00, 56.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.0266, Train Accuracy: 49.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEvaluating: 100%|██████████| 5483/5483 [00:32<00:00, 169.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation Loss: 1.0560, Validation Accuracy: 45.96%\n",
      "EarlyStopping counter: 1/7\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [4/20]: 100%|██████████| 53851/53851 [16:12<00:00, 55.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.0300, Train Accuracy: 49.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEvaluating: 100%|██████████| 5483/5483 [00:36<00:00, 149.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation Loss: 1.0606, Validation Accuracy: 45.96%\n",
      "EarlyStopping counter: 2/7\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [5/20]: 100%|██████████| 53851/53851 [16:23<00:00, 54.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.0316, Train Accuracy: 48.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEvaluating: 100%|██████████| 5483/5483 [00:36<00:00, 152.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation Loss: 1.0632, Validation Accuracy: 45.96%\n",
      "EarlyStopping counter: 3/7\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [6/20]: 100%|██████████| 53851/53851 [16:21<00:00, 54.84batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.0328, Train Accuracy: 48.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEvaluating: 100%|██████████| 5483/5483 [00:36<00:00, 151.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation Loss: 1.0632, Validation Accuracy: 45.96%\n",
      "EarlyStopping counter: 4/7\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [7/20]: 100%|██████████| 53851/53851 [16:24<00:00, 54.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.0328, Train Accuracy: 48.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEvaluating: 100%|██████████| 5483/5483 [00:36<00:00, 150.52batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation Loss: 1.0632, Validation Accuracy: 45.96%\n",
      "EarlyStopping counter: 5/7\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [8/20]: 100%|██████████| 53851/53851 [16:21<00:00, 54.85batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.0451, Train Accuracy: 46.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEvaluating: 100%|██████████| 5483/5483 [00:37<00:00, 145.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation Loss: 1.0539, Validation Accuracy: 45.96%\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [9/20]: 100%|██████████| 53851/53851 [16:22<00:00, 54.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.0453, Train Accuracy: 46.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEvaluating: 100%|██████████| 5483/5483 [00:37<00:00, 145.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation Loss: 1.0539, Validation Accuracy: 45.96%\n",
      "EarlyStopping counter: 1/7\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [10/20]: 100%|██████████| 53851/53851 [16:19<00:00, 54.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.0453, Train Accuracy: 46.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEvaluating: 100%|██████████| 5483/5483 [00:35<00:00, 154.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation Loss: 1.0539, Validation Accuracy: 45.96%\n",
      "EarlyStopping counter: 2/7\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [11/20]: 100%|██████████| 53851/53851 [16:19<00:00, 54.99batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.0453, Train Accuracy: 46.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEvaluating: 100%|██████████| 5483/5483 [00:34<00:00, 156.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation Loss: 1.0539, Validation Accuracy: 45.96%\n",
      "EarlyStopping counter: 3/7\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training [12/20]:  73%|███████▎  | 39173/53851 [11:52<04:27, 54.95batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m DEVICE = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m torch.cuda.empty_cache()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train_losses, train_accs, val_losses, val_accs = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                                                             \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                                                             \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                                                             \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, patience)\u001b[39m\n\u001b[32m     69\u001b[39m outputs = model(images)\n\u001b[32m     70\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m optimizer.step()\n\u001b[32m     75\u001b[39m running_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\longt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    513\u001b[39m         Tensor.backward,\n\u001b[32m    514\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m         inputs=inputs,\n\u001b[32m    520\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\longt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    284\u001b[39m     retain_graph = create_graph\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\longt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    767\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "train_losses, train_accs, val_losses, val_accs = train_model(model, \n",
    "                                                             train_loader, val_loader, \n",
    "                                                             criterion, optimizer, scheduler, \n",
    "                                                             NUM_EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "829afcde",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m     plt.tight_layout()\n\u001b[32m     55\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m plot_training_results(\u001b[43mtrain_losses\u001b[49m, train_accs, val_losses, val_accs)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_training_results(train_losses, train_accs, val_losses, val_accs):\n",
    "    \"\"\"\n",
    "    Vẽ biểu đồ kết quả huấn luyện: loss và accuracy cho train và validation.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_losses: List các giá trị loss của train qua các epoch\n",
    "    - train_accs: List các giá trị accuracy của train qua các epoch\n",
    "    - val_losses: List các giá trị loss của validation qua các epoch\n",
    "    - val_accs: List các giá trị accuracy của validation qua các epoch\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    # Tạo figure với 2x2 subplot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Subplot 1: Train Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Subplot 2: Train Accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, train_accs, 'g-', label='Train Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Subplot 3: Validation Loss\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Subplot 4: Validation Accuracy\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, val_accs, 'm-', label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Điều chỉnh layout và hiển thị\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_results(train_losses, train_accs, val_losses, val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ac6557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1703ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)  # [B, seq_len, n_features]\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_x)  # Expecting [B, num_classes]\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    target_names = ['BUY', 'SELL', 'HOLD']\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07981475",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, train_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c787c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, val_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a96c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, test_loader, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7055460,
     "sourceId": 11294529,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
